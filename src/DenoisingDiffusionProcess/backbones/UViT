from torch import nn

class BlockMLP(nn.Module):
    
    def __init__(self):
        super().__init__()
        
        self.norm=nn.LayerNorm(dim)
        
    def forward(self,x, emb, expansion_factor=4):
        b, c, hw = x.shape
        x = self.norm(x)
        mlp_h=Dense(x,expansion_factor*c)
        emb_scale=DenseGeneral(emb,mlp_h.shape[2:])
        emb_shift=DenseGeneral(emb,mlp_h.shape[2:])

class SelfAttention(nn.Module):
    
    def __init__(self):
        super().__init__()

class TransformerBlock(nn.Module):
    
    def __init__(self):
        super().__init__()

class ResNetBlock(nn.Module):
    
    def __init__(self):
        super().__init__()

class UViT(nn.Module):
    
    def __init__(self):
        super().__init__()
        
    def embed_logsnr(self,logsnr):
        return logsnr
        
    def forward(self, x, logsnr):
        b,c,h,w=x.shape
        
        emb=self.embed_logsnr(logsnr)
        
        # Down path
        for i_level in range(len(self.num_res_blocks)):
            for i_block in range(self.num_res_blocks[i_level]):
                last_h=resnet_block(last_h, emb)
                hs.append(last_h)
                
            last_h=downsample(last_h, self.base_channels*self.channel_multiplier[i_level+1])
        
        
        # The Transformer
        last_h=last_h.reshape(b,c,h*w)
        last_h+=param("pos_emb")
        
        for _ in range(self.num_transformer_blocks):
            last_h=transformer_block(last_h,text_emb,emb)
        last_h=last_h.reshape(b,c,h,w)
                
        # Up Path
        for i_level in reversed(range(len(self.num_res_blocks))):
            last_h=upsample(last_h, self.base_channels*self.channel_multpiplier[i_level])
            
            for i_block in  range(self.num_res_blocks[i_level]):
                last_h=resnet_block(last_h, emb, skip_h=hs.pop())
                
        out=ProjectOutput(last_h, C)
        return out